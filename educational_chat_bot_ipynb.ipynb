import whisper
from groq import Groq
from gtts import gTTS
import gradio as gr
import os
os.environ["GROQ_API_KEY"] = 	"gsk_...2du1"
# Initialize the Whisper model
whisper_model = whisper.load_model("base")

# Initialize Groq client with API key
client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

# Function to process audio, interact with LLM, and convert response to speech
def process_audio(audio_file):
    # Step 1: Transcribe the audio input using Whisper
    transcription = whisper_model.transcribe(audio_file)["text"]
    print(f"Transcription: {transcription}")

    # Step 2: Query Groq LLM with the transcribed text
    response = client.chat.completions.create(
        messages=[
            {"role": "user", "content": transcription}
        ],
        model="llama3-8b-8192",
    )

    # Extract response from Groq LLM
    llm_response = response.choices[0].message.content
    print(f"LLM Response: {llm_response}")

    # Step 3: Convert LLM-generated text into speech using gTTS
    tts = gTTS(text=llm_response, lang='en')
    output_audio = "response.mp3"
    tts.save(output_audio)

    # Return the transcription and generated speech audio
    return transcription, output_audio
import gradio as gr

# Define your function to handle input and generate a response (as per your chatbot logic)
def process_audio(audio):
    transcription = "Transcribed text will appear here."  # Replace with actual transcription logic
    response_text = "Chatbot response will appear here."  # Replace with actual chatbot response logic
    # Convert response_text to audio (using gTTS or another tool)
    response_audio = None  # Replace with actual audio generation logic
    return transcription, response_audio

# Create the Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Spoken English Practice Chatbot")
    # Use Gradio's updated Audio component without the 'source' argument
    audio_input = gr.Audio(type="filepath", label="Speak Now")
    output_text = gr.Textbox(label="Transcription")
    output_audio = gr.Audio(label="Response Speech")

    # Link the components to the processing function
    audio_input.change(process_audio, inputs=[audio_input], outputs=[output_text, output_audio])

# Launch the interface
demo.launch()

import gradio as gr

# Define the function to process audio input and generate a response
def process_audio(audio):
    transcription = "Transcribed text will appear here."  # Replace with actual transcription logic
    response_text = "Chatbot response will appear here."  # Replace with actual chatbot response logic
    response_audio = None  # Replace with audio generation logic (e.g., gTTS)
    return transcription, response_audio

# Create the Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Spoken English Practice Chatbot")
    
    # Audio input for microphone
    audio_input = gr.Audio(type="filepath", label="Speak Now")

    # Textbox for transcription output
    output_text = gr.Textbox(label="Transcription")

    # Audio output for chatbot response
    output_audio = gr.Audio(label="Response Speech")

    # Button to trigger the processing function
    submit_button = gr.Button("Submit")

    # Link the button click to the function
    submit_button.click(
        process_audio, 
        inputs=[audio_input], 
        outputs=[output_text, output_audio]
    )

# Launch the app
demo.launch()

# Launch the Gradio app
demo.launch()
